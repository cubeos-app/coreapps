# Ollama AI Model Server
# Port: 6030 (API)
# Runs local LLMs for CubeOS AI Assistant
services:
  ollama:
    image: ollama/ollama:latest
    container_name: cubeos-ollama
    restart: unless-stopped
    privileged: true
    ports:
      - "6030:11434"
    volumes:
      - ../appdata:/root/.ollama
    environment:
      - TZ=${TZ:-UTC}
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
    env_file:
      - /cubeos/config/defaults.env
    security_opt:
      - apparmor:unconfined
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/127.0.0.1/11434'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - "cubeos.type=ai"
      - "cubeos.category=ai"
      - "cubeos.port=6030"
      - "cubeos.description=Ollama AI Model Server"
