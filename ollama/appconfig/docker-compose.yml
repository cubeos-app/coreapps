# Ollama AI Model Server
# ═══════════════════════════════════════════════════════════════════════════════
# DEPLOYMENT: Docker Swarm stack
# Port: 6030 (API, maps from internal 11434)
# Runs local LLMs for CubeOS AI Assistant
#
# Deploy: docker stack deploy -c docker-compose.yml --resolve-image=never ollama
# ═══════════════════════════════════════════════════════════════════════════════

services:
  ollama:
    image: ollama/ollama:latest
    hostname: cubeos-ollama
    ports:
      - target: 11434
        published: 6030
        protocol: tcp
        mode: host
    volumes:
      - ../appdata:/root/.ollama
    environment:
      - TZ=${TZ:-UTC}
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
    env_file:
      - /cubeos/config/defaults.env
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      labels:
        - "cubeos.type=ai"
        - "cubeos.category=ai"
        - "cubeos.port=6030"
        - "cubeos.fqdn=ollama.cubeos.cube"
        - "cubeos.deployment=swarm"
        - "cubeos.description=Ollama AI Model Server"
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/127.0.0.1/11434'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

networks:
  default:
    external: true
    name: cubeos-network
# Swarm deploy zo  1 feb 2026  1:52:10 CET
# redeploy zo  1 feb 2026  2:02:35 CET
